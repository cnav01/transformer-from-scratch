# Transformer from Scratch (PyTorch)

An implementation of the original Transformer model from the paper ["Attention Is All You Need"](https://arxiv.org/abs/1706.03762) by Vaswani et al.

##  Project Overview
I built this project to understand the inner workings of Self-Attention and the Encoder-Decoder architecture. It includes a full implementation of:
- Multi-Head Attention mechanism
- Positional Encodings (Sinusoidal/RoPE)
- Position-wise Feed-Forward Networks
- Custom training loop in PyTorch

##  Structure
- `model/`: Contains the modularized model architecture.
- `train.py`: Script for training the model on [Your Dataset Name].
- `inference.py`: Script for testing the model's generation capabilities.

## üõ†Ô∏è Installation
1. Clone the repo:
   ```bash
   git clone [https://github.com/yourusername/transformer-from-scratch.git](https://github.com/yourusername/transformer-from-scratch.git)
